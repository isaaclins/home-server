---
description: 
globs: 
alwaysApply: false
---
# Home Server Project Plan

This document outlines the requirements and plan for creating a home server with API capabilities, including Ollama LLM integration.

## Project Repository and Deployment Strategy

*   **Central Repository:** This Git repository (currently named `home-server`) will serve as the single source of truth for the entire home server project. It will contain all code, configurations, and documentation.
*   **Contents:**
    *   All source code for the custom APIs, web interfaces (homepage, file upload UI, dashboard, etc.).
    *   The `Dockerfile` and any associated files (e.g., scripts, base configurations) required to build the primary Docker image for the home server.
    *   Service configurations, such as `docker-compose.yml` files if used for orchestrating the main application and potentially other services like a reverse proxy or monitoring tools.
    *   Environment variable templates or examples (`.env.example`).
    *   This `home-server-plan.mdc` rule file and all other project documentation and setup guides.
*   **Deployment Workflow:**
    1.  Develop and test features within this repository.
    2.  Build a Docker image using the `Dockerfile` contained herein. This image will package the API server and all its dependencies.
    3.  Push the built Docker image to a container registry (e.g., Docker Hub, GitHub Container Registry, or a private registry) if desired, or transfer it directly to the server.
    4.  Deploy and run this Docker image on the target Linux server PC.
*   **Configuration Management:** Configurations needed by the running Docker container (e.g., API keys, specific Ollama model paths if not hardcoded, database connection strings) should be managed primarily through environment variables passed to the Docker container at runtime. The initial setup for the master admin user (see Section II) will also fall under this, storing credentials securely (e.g., hashed password in a persistent volume).
*   **Backup Alignment:** The "Automated Backups (Code & Configuration)" (Section VIII) strategy, focusing on backing up this repository to GitHub, directly supports this centralized approach, ensuring the core logic and setup can be restored.

## I. Operating System Setup

*   The server will be set up on a PC currently running Windows.
*   It will be dual-booted with a Linux server distribution.
*   The Linux distribution will be terminal-only (no GUI).
*   **Ollama Installation:** The server setup process (potentially within the main Docker image build or an initial setup script) must include provisions for automatically downloading and installing the official Ollama service binary to ensure a "plug and play" experience. The Ollama service should be configured to start automatically.

## II. Core API and Homepage

*   **URL Structure and Accessibility:**
    *   The main web UIs (Homepage, Admin Panel, Dashboard) will be accessible at the root of the server's primary domain (e.g., `https://server.isaaclins.com/`).
    *   All core API endpoints will be grouped under a common base path (e.g., `https://server.isaaclins.com/api/`).
    *   This structure will be managed by a reverse proxy handling SSL termination and request routing.
*   **Homepage / Admin Panel UI:**
    *   **Authentication:** Access to all web UIs will be protected via HTTP Basic Auth using credentials of registered users. Sensitive operations will require admin privileges.
    *   **Initial Setup:** Upon first run of the Docker image, an interactive prompt (or a secure setup script) will guide the creation of a master/root admin user (username and password). This password will be hashed and stored securely in a persistent volume.
    *   **Main Interface:** The primary web interface will serve as both a dashboard (for all users, showing allowed services) and an admin panel (for admin users).
    *   **Admin Panel Functionalities (for admin users):**
        *   User management (create, list, delete non-admin users).
        *   API permission management (assign specific API access rights to non-admin users).
        *   Overview of server status and links to other managed services (like the PaaS dashboard if separate).
    *   **Standard User View:** Non-admin users will see a simplified homepage linking to the APIs and services they have been granted access to.

## III. API Functionalities

The server will expose several APIs. Key functionalities include:

### A. File Hosting

*   **UI Upload:**
    *   Users should be able to upload files via a web-based user interface.
    *   The UI should allow users to specify a retention period for the uploaded file.
    *   Retention options: 10 minutes, 1 hour, 10 hours, 10 days, or indefinitely.
*   **API Upload:**
    *   An API endpoint should be available for programmatic file uploads.
    *   A simple method like multipart/form-data for raw files or a JSON payload with a base64 encoded string for the file content should be supported.
    *   The API request must also allow specifying the retention period.
*   **URL Generation:**
    *   Upon successful upload (either via UI or API), the system should return a unique, shareable URL for accessing the file.
*   **Automatic Deletion:**
    *   Files should be automatically deleted from the server once their specified retention period expires (unless set to indefinite).
*   **Security Note:** Ensure proper validation of file types and sizes to prevent abuse.

### B. Ollama LLM Management

*   **Interaction Method:** All interactions with the Ollama service (pulling models, starting, stopping, chatting) will be performed by the custom API by making requests to Ollama's local REST API (documented at [https://github.com/ollama/ollama/blob/main/docs/api.md](mdc:https:/github.com/ollama/ollama/blob/main/docs/api.md)). This is preferred over CLI wrappers for robustness and ease of integration.

This is a critical component of the server. The API must provide endpoints for the following actions based on Ollama's API capabilities:

1.  **Pull and Start an Ollama LLM:**
    *   An endpoint to download (if not already present) and start a specified Ollama Large Language Model.
    *   The request should specify the model name/tag.
    *   The API should confirm successful startup.
2.  **Chat with an Ollama LLM:**
    *   An endpoint to send a prompt to a currently running Ollama LLM and receive its response.
    *   The request should specify which model to interact with (if multiple can run, though initially likely one at a time).
3.  **Stop an Ollama LLM:**
    *   An endpoint to stop a currently running Ollama LLM.
    *   The request should specify the model to stop.
    *   The API should confirm successful shutdown.

### C. PaaS / Container Management (Docker)

*   **Objective:** Allow running arbitrary applications packaged as Docker images on the server.
*   **Image Source:**
    *   Ability to pull Docker images from public repositories (e.g., Docker Hub) by specifying the image name and tag.
    *   (Optional Future Enhancement): Ability to upload a `Dockerfile` or a Docker image tarball to build/load custom images.
*   **Container Deployment and Networking:**
    *   **User Input:** When deploying a container, the user will specify the container's internal port that the application listens on (e.g., `3000`).
    *   **Host Port Assignment:** The PaaS system will automatically find an unused port on the host machine and map the specified container port to this auto-assigned host port.
    *   **Service URL:** The system will then provide the user with the access URL for their application, in the format `https://server.isaaclins.com:<auto-assigned-host-port>` (assuming `server.isaaclins.com` is the server's domain name).
*   **Container Lifecycle Management (API Endpoints):**
    *   **Deploy/Run Container:** Start a new container from a specified Docker image. This should allow for:
        *   Assigning a name to the container.
        *   Specifying the container's internal application port (for automatic host port mapping).
        *   Setting environment variables.
        *   Mounting volumes (with careful consideration for security and host file system access).
    *   **List Containers:** View all containers managed by this PaaS system (both running and stopped).
    *   **Inspect Container:** Get detailed information about a specific container (status, ports, logs, etc.).
    *   **Stop Container:** Gracefully stop a running container.
    *   **Start Container:** Start a previously stopped container.
    *   **Remove Container:** Delete a stopped container.
    *   **View Container Logs:** Stream or retrieve logs from a running or stopped container.
*   **Resource Management for PaaS Containers:**
    *   The system must allow setting resource limits (CPU, memory) when deploying a container to prevent any single PaaS application from overwhelming the server. Sensible default limits should be applied if not specified by the user.
    *   PaaS containers will generally run at a standard OS priority, lower than core server processes or active Ollama tasks, unless specifically adjusted by an admin for critical applications.
*   **Security Considerations:**
    *   Running arbitrary Docker images poses security risks. This feature should be implemented with strong security measures in mind:
        *   Potentially restricting which users can deploy containers.
        *   Careful validation of image sources.
        *   Network isolation if possible.
        *   Regularly update Docker and scan images for vulnerabilities.
*   **UI Integration (Optional):**
    *   A section in the main homepage could provide a simple UI for managing these containers, potentially restricted to admin users or based on specific PaaS permissions.

### D. User Management & Authorization API

*   **Objective:** Provide API endpoints to support the admin panel functionalities and enforce access control.
*   **Admin-Only Endpoints:**
    *   Create user (username, password).
    *   List users.
    *   Delete user.
    *   Grant/Revoke API permissions for a user (e.g., map user to specific API endpoint groups or individual endpoints).
    *   List permissions for a user.
*   **General Endpoints (for user self-service, if needed later):**
    *   (Future) Change own password.
*   **Authentication & Authorization Layer:** All API endpoints (including File Hosting, Ollama, PaaS) must pass through an authentication layer (verifying the user, e.g., via HTTP Basic Auth credentials or an API key derived from them) and an authorization layer (checking if the authenticated user has permission to access the requested resource/action based on rules set in the admin panel).

## IV. Safeguards and Error Handling

The server should implement several safeguards:

*   **Idle Model Timeout (Ollama):**
    *   If an Ollama LLM has not received any requests for a configurable period (e.g., 10 minutes), it should be automatically stopped (or unloaded) to conserve resources.
*   **Model Not Running Error (Ollama):**
    *   If a user attempts to chat with an Ollama LLM that has not been pulled or started, the API should return an informative error message (e.g., "Model not found or not currently running. Please pull/start the model first.").
*   **Overall Resource Management & Prioritization Strategy:**
    *   **Objective:** Maintain server stability and responsiveness by prioritizing critical services and managing resource allocation under load.
    *   **Service Tiers & Priority (Highest to Lowest):
        1.  **Core System & API:** Main web server, reverse proxy, auth system, user DB. (Highest OS priority, e.g., via `nice` values or `cgroups`).
        2.  **Active Ollama LLM Tasks:** Ollama service when actively processing. (High priority during active use).
        3.  **Active PaaS Containers:** User-deployed Docker applications. (Standard priority, subject to configured resource limits).
        4.  **File Hosting Service:** Standard priority, focus on I/O efficiency.
        5.  **Background & Management Tasks:** Dashboard, backups, idle Ollama. (Lower priority).
    *   **Mechanisms:**
        *   **PaaS Container Limits:** Enforce CPU/memory limits for PaaS containers.
        *   **OS-Level Priority:** Utilize Linux `nice` levels or `cgroups` to give higher priority to core server components.
        *   **Health Checks & Auto-Restarts:** Implement for critical services (Core API, Reverse Proxy, Ollama).
        *   **Monitoring & Alerting:** Leverage the dashboard (Section VII) for resource visibility and consider alerts for critical conditions (e.g., low disk space, sustained high CPU).
        *   **Graceful Degradation:** If under extreme load, the system should aim to keep core administrative functions responsive, potentially by rate-limiting or queuing requests for lower-priority services.
    *   Consider limitations on how many Ollama models can run simultaneously if the hardware is constrained, providing clear feedback to the user.

## V. Technology Stack (To Be Decided)

*   **Linux Distribution:** (e.g., Ubuntu Server, Debian)
*   **Reverse Proxy:**
    *   A reverse proxy (e.g., Nginx, Traefik, Caddy) will be essential.
    *   Responsibilities: SSL termination for the primary domain (e.g., `https://server.isaaclins.com`), routing requests to `/` to the main application's UI, and requests to `/api/` to the main application's API handlers. It might also handle PaaS container exposure if a more advanced setup is chosen over direct port mapping.
*   **Web Server/Framework for API:** (e.g., Python with Flask/FastAPI, Node.js with Express) - This will serve the content for `/` and `/api/` paths, behind the reverse proxy.
*   **Authentication Mechanism:**
    *   HTTP Basic Auth for accessing web UIs (Homepage, Admin Panel, Dashboard).
    *   API requests will also use HTTP Basic Auth initially, or could use API keys generated for users. Passwords must be stored hashed (e.g., using bcrypt or scrypt).
    *   A simple user database (e.g., SQLite file stored in a persistent Docker volume) will be required to store user credentials (hashed passwords) and their assigned permissions.
*   **Authorization Layer:** A component within the API framework to manage and enforce user permissions for different API endpoints/features.
*   **Ollama Interaction:** The custom API will communicate with the Ollama service exclusively via its local REST API. The Ollama service itself will be installed on the host (or within a dedicated container managed by the main application) and its API endpoint (typically `http://localhost:11434`) will be accessed by the server's backend.
*   **Resource Control Mechanisms (Host OS Level):**
    *   Consider using Linux `cgroups` and `nice`/`renice` commands as part of service management scripts or Docker container configurations to enforce the resource prioritization strategy.

## VI. Documentation

*   The API endpoints should be well-documented.
*   The setup process for the server itself should be documented.

## VII. Comprehensive Dashboard & Monitoring

*   **Objective:** Provide a centralized view of the server's health, resource utilization, and status of deployed services.
*   **Key Metrics to Monitor:**
    *   **System Resources:** CPU usage (overall and per core), RAM consumption, disk space (total, used, free for key partitions), network I/O (bandwidth usage, active connections).
    *   **Service Status:**
        *   Core API: Up/down status, request latency, error rates.
        *   Ollama Service: Status (running, stopped), current model loaded, resource consumption by Ollama.
        *   Deployed Docker Containers: Status (running, stopped, restarting), resource consumption (CPU, memory if available per container), port mappings, logs access.
        *   File Hosting Service: Number of files hosted, total storage used by files.
*   **Display:**
    *   Should be accessible via the password-protected main homepage or a dedicated sub-page.
    *   Use clear visualizations like graphs, gauges, and status indicators.
*   **Potential Tools/Integrations:**
    *   Consider using existing dashboarding solutions like `Netdata` (easy setup, real-time), `Grafana` with `Prometheus` (powerful, customizable), or building a custom dashboard within the main API's web framework.
    *   The chosen solution should be lightweight enough not to overburden the server.

## VIII. Automated Backups (Code & Configuration)

*   **Objective:** Ensure the server's codebase, API definitions, service configurations, and setup scripts can be easily restored on the same or a different machine in case of hardware failure or system corruption. This backup method is **specifically for the application and its configuration, not for user-generated data**.
*   **Scope of Backup:**
    *   **Primary Focus:** All source code for the custom API, web interface, and any helper scripts.
    *   **Configurations:** Server configurations (e.g., web server configs, Docker daemon settings if customized), Ollama service configurations (if any beyond default), configurations for deployed Docker applications (e.g., `docker-compose.yml` files, environment variable files), user database structure (if applicable, but not its live content if large).
    *   **Documentation:** Including this `home-server-plan.mdc` file and other setup/API documentation.
*   **Exclusions (from THIS GitHub-based backup strategy):**
    *   **All user-uploaded files:** This includes files from the File Hosting service, regardless of their retention period (timed or "infinite"). The "infinite" setting pertains to the file hosting service's auto-deletion policy, not its inclusion in this code/config backup. If backup of these uploaded files is required, a separate User Data Backup Strategy must be defined and implemented (e.g., regular snapshots to a different storage medium).
    *   **Live user database content:** While the schema might be versioned, the live content of the user database (holding user accounts, permissions) if it grows large, should be part of a separate User Data Backup Strategy. For small SQLite databases, the file itself might be included if carefully managed.
    *   **Downloaded Ollama models:** These can be re-pulled from their source.
    *   **Extensive/Large log files:** These should be managed by log rotation, and critical summarized logs could be backed up separately if needed.
*   **Backup Destination & Method:**
    *   **GitHub Repository:** Utilize a private GitHub repository to store all version-controlled code and configuration files.
    *   **Automation:** Set up automated commits and pushes to this repository.
        *   This could be achieved via cron jobs running `git add .`, `git commit -m "Automated backup [timestamp]"`, and `git push`.
        *   Frequency: Daily or on significant changes (e.g., after deploying a new feature).
*   **Restore Process:**
    *   The goal is to be able to clone the GitHub repository on a new (Linux) machine, run a setup script (also version-controlled), and have the server operational again (Ollama models and user files would still need to be re-pulled/re-uploaded as per their original design).
*   **Security:**
    *   Ensure no sensitive credentials (API keys, passwords for external services) are hardcoded directly into version-controlled files. Use environment variables or a separate, securely managed secrets system.

This rule file will guide the development and ensure all key features are addressed.
