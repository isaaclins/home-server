# Ollama Chat Page - Initial Component Setup

This document outlines the initial steps taken to set up the necessary Shadcn UI components for the Ollama Chat page.

## Steps Taken:

1.  **Identified Missing Components**: The chat page requires a `Textarea` for user input and a `Select` component for model selection. These were not yet available in the project's `components/ui` directory.

2.  **Obtained `Textarea` Component Source Code**:

    - Searched the web for the official Shadcn UI documentation for the `Textarea` component.
    - Located the component's source code on the Shadcn UI GitHub repository: `https://github.com/shadcn-ui/ui/blob/main/apps/www/registry/default/ui/textarea.tsx`.
    - Copied the raw TypeScript/JSX code for the `Textarea` component.

3.  **Created `textarea.jsx`**:

    - Created a new file at `docker-data/frontend/components/ui/textarea.jsx`.
    - Pasted the copied `Textarea` source code into this file.

4.  **Obtained `Select` Component Source Code**:

    - Searched the web for the official Shadcn UI documentation for the `Select` component.
    - Located the component's source code on the Shadcn UI GitHub repository: `https://github.com/shadcn-ui/ui/blob/main/apps/www/registry/default/ui/select.tsx`.
    - Copied the raw TypeScript/JSX code for the `Select` component and its related sub-components (SelectGroup, SelectValue, SelectTrigger, SelectContent, SelectLabel, SelectItem, SelectSeparator, SelectScrollUpButton, SelectScrollDownButton).

5.  **Created `select.jsx`**:
    - Created a new file at `docker-data/frontend/components/ui/select.jsx`.
    - Pasted the copied `Select` source code into this file.

## Reason for Manual Copying:

As per the project's `shadcn-ui-integration-guide`, the `npx shadcn-ui@latest add <component>` command should not be used. Instead, component source code must be obtained manually from the official documentation or repository to ensure correctness and avoid potential issues with the CLI tool in the project's Dockerized environment.

With these components added, the next step is to start building the UI structure of the `ollama-chat.js` page.

## Building the Basic Chat UI (`ollama-chat.js`)

1.  **Imported Necessary Components**:

    - Imported `useState` and `useEffect` from React for state management and side effects.
    - Imported `useRouter` from `next/router` for navigation.
    - Imported `Button`, `Card` (and its sub-components), `Input`, `Textarea`, and `Select` (and its sub-components) from the project's UI library (`@/components/ui/...`).
    - Imported `useAuthContext` to access user authentication state and roles.

2.  **Initialized State Variables**:

    - `message`: To store the current message in the text input.
    - `chatHistory`: To store the list of chat messages (sender and text).
    - `selectedModel`: To store the ID of the currently selected Ollama model.
    - `models`: To store the list of available Ollama models (initially populated with placeholder data).

3.  **Implemented Basic Authentication/Authorization Check**:

    - In `useEffect`, added logic to redirect to `/login` if the user is not logged in.
    - Added placeholder comments for future implementation of role-based access control (`_ollama_user` role) and fetching models from the API.
    - Populated `models` state with a few placeholder models and set a default `selectedModel`.

4.  **Created `handleSendMessage` Function**:

    - A basic function to handle sending messages. Currently, it adds the user's message to `chatHistory`, clears the input, and simulates a bot echo response after a short delay.
    - Added a placeholder comment for future implementation of actual API calls to Ollama.

5.  **Structured the JSX for the Chat Interface**:

    - Used a `Card` component as the main container for the chat interface.
    - Added a `CardHeader` with a title and description.
    - In `CardContent`:
      - Added a `Select` component to allow users to choose an Ollama model from the `models` state.
      - Added a `div` to display the `chatHistory`. Messages are styled differently based on the sender (`user` or `bot`).
      - Added a `Textarea` for message input and a `Button` to send messages. Pressing Enter (without Shift) in the textarea also triggers `handleSendMessage`.
    - Conditionally rendered a `CardFooter` with "Admin Controls" (placeholder buttons for pulling and deleting models) if the `isAdmin` flag from `useAuthContext` is true.

6.  **Styling**: Applied basic Tailwind CSS classes for layout and appearance.

This provides a foundational UI for the chat page. The next steps will involve integrating with the actual Ollama API, implementing user role checks, and saving/retrieving chat history.

## Setting Up API Route for Ollama Models

1.  **Created API Route File**:

    - Created a new file at `docker-data/frontend/pages/api/ollama/models.js`.
    - This route will be responsible for providing the list of available Ollama models.

2.  **Implemented `GET` Handler**:

    - Added an `async function GET(request)` to handle GET requests to this endpoint.
    - Created a `fetchOllamaModels` async function that currently returns a hardcoded array of model objects (e.g., `llama2`, `mistral`, `codellama`, `llava`). Each object includes `id`, `name`, and `details`.
    - The `GET` handler calls `fetchOllamaModels` and returns the models as a JSON response using `NextResponse.json()`.
    - Included basic error handling that logs an error and returns a 500 status if fetching fails.
    - Added a `TODO` comment to replace the placeholder data with an actual API call to a local Ollama instance (e.g., `http://localhost:11434/api/tags`).

3.  **Updated `ollama-chat.js` to Fetch Models from API**:
    - Modified the `useEffect` hook in `docker-data/frontend/pages/ollama-chat.js`.
    - Created an `async` function `getModels` inside `useEffect`.
    - `getModels` now uses `fetch('/api/ollama/models')` to retrieve the list of models.
    - Upon successful fetch, it updates the `models` state with the received data and sets `selectedModel` to the ID of the first model in the list as a default.
    - Includes error handling for the fetch request, logging errors to the console and clearing the `models` state if the fetch fails.
    - The placeholder `setModels([...])` and `setSelectedModel(...)` calls were removed from `useEffect`.

## Implementing Chat Functionality with Streaming

1.  **Created API Route for Chat (`/api/ollama/chat.js`)**:

    - Created a new file at `docker-data/frontend/pages/api/ollama/chat.js`.
    - Implemented an `async function POST(request)` to handle chat messages.
    - The handler expects `model`, `messages` (an array of previous chat messages for context), and `stream` (boolean) in the JSON request body.
    - **Simulated Streaming Response**: If `stream` is true:
      - It uses a `ReadableStream` to send back chunks of a simulated response.
      - The placeholder response is an echo of the last user message, split into words, with each word sent as a separate chunk.
      - Each chunk is formatted as a JSON object similar to Ollama's streaming output: `{ model, created_at, message: { role: "assistant", content: chunk }, done: false }`.
      - A final "done" message `{ model, created_at, done: true }` is sent when the stream ends.
      - The response headers are set to `Content-Type: application/x-ndjson` and `Transfer-Encoding: chunked`.
    - **Simulated Non-Streaming Response**: If `stream` is false, it returns a full, non-streamed JSON response with the echoed message.
    - Includes basic error handling and validation for required fields.
    - Added a `TODO` comment to replace the simulation with actual calls to the Ollama API (`/api/chat`).

2.  **Updated `handleSendMessage` in `ollama-chat.js`**:
    - Converted `handleSendMessage` to an `async` function.
    - The user's message is immediately added to `chatHistory` for a responsive UI, and the input field is cleared.
    - The existing `chatHistory` is mapped to the format expected by the Ollama API (array of objects with `role` and `content`) and the new user message is appended.
    - A `POST` request is made to `/api/ollama/chat` with `model`, `messages` (the contextual history), and `stream: true`.
    - **Stream Processing**:
      - If the response is `ok` and has a `body`, it uses `response.body.pipeThrough(new TextDecoderStream()).getReader()` to read the stream.
      - A placeholder entry for the bot's response is added to `chatHistory`.
      - It reads chunks from the stream in a `while` loop.
      - Each received chunk (which can contain multiple newline-separated JSON objects) is processed:
        - Lines are split and parsed as JSON.
        - If a chunk contains `message.content`, this content is appended to an `accumulatedResponse`.
        - The last entry in `chatHistory` (the bot's placeholder) is updated with the `accumulatedResponse` to show the streaming text.
        - If a chunk contains `done: true`, the streaming is considered complete, and the loop is exited.
      - Error handling is included for parsing stream chunks.
    - A fallback is included to handle non-streaming responses (though `stream: true` is always requested for now).
    - Error handling for the API request itself is included, adding an error message to the chat if the request fails.
